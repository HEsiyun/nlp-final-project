# nlp-final-project
## LSTM model
***LSTM.ipynb:*** The code for the LSTM model is available in LSTM.ipynb. It demonstrates the implementation of a bidirectional LSTM, with the single-layer and multi-layer LSTM variants commented out. <br>
## BERT
***BERT.ipynb:*** The implementation of BERT model with batch size = 64, learning rate = 2e-5, and epochs = 3. <br>
***BERT_lr3.ipynb:*** The implementation of BERT model with batch size = 64, learning rate = 3e-5, and epochs = 3. <br>
***BERT_lr5.ipynb:*** The implementation of BERT model with batch size = 64, learning rate = 5e-5, and epochs = 3. <br>
***BERT_batch32.ipynb:*** The implementation of BERT model with batch size = 32, learning rate = 2e-5, and epochs = 3. <br>
***BERT_batch32_lr3.ipynb:*** The implementation of BERT model with batch size = 32, learning rate = 3e-5, and epochs = 3. <br>
These implementations are identical except for differences in hyperparameter settings. All files are retained to observe the results under various configurations.
## BitLineared BERT
***BitLinear_Bert.ipynb:*** The implementation of BitLineared BERT model with batch size = 64, learning rate = 2e-5, and epochs = 3. <br>
***BitLinear_Bert_batch32.ipynb:*** The implementation of BitLineared BERT model with batch size = 32, learning rate = 2e-5, and epochs = 3. <br>
These implementations are identical except for differences in hyperparameter settings. All files are retained to observe the results under various configurations.
## Data preprocess
***data_preprocess:*** The data preprocessing steps. <br>
## References
[1] J. Brownlee, “Sequence Classification with LSTM recurrent neural networks in python with keras,” MachineLearningMastery.com, https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/ <br>
[2]  “Sentiment analysis with Bert and transformers by hugging face using pytorch and python,” Curiousily, https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/ <br>
[3] Kyegomez, “Kyegomez/BITNET: Implementation of ‘Bitnet: Scaling 1-bit transformers for large language models’ in pytorch,” GitHub, https://github.com/kyegomez/BitNet 